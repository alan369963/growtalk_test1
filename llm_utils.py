"""
llm_utils.py

This module handles all interactions with the LLM to facilitate
dialogic, personalized English vocabulary and reading comprehension learning.

Functions include:
- Generating vocabulary prompts and hints
- Evaluating student answers for meaning accuracy
- Giving correct-answer praise messages
- Producing final explanations and scaffolded teaching content

Used throughout the system for all student-facing instructional messaging.
"""

from openai import OpenAI
import config
import sheet_utils

client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=config.OPENROUTER_API_KEY,
)

# General system prompt for GrowTalk
system_prompt_reading = f"""ä½ æ˜¯ä¸€ä½å°ˆç‚ºé¦™æ¸¯ä¸­å­¸ç”Ÿè¨­è¨ˆçš„ AI è‹±æ–‡é–±è®€è€å¸«ã€‚ä½ ä¸»è¦ä»¥å»£æ±è©±æ•™è‹±æ–‡ï¼Œåªåœ¨éœ€è¦æå‡ºè‹±æ–‡é–±è®€å•é¡Œã€è¬›è§£è‹±æ–‡è©èªã€å¥å¼æˆ–ä¾‹å¥æ™‚æ‰ç”¨è‹±æ–‡ï¼Œä¸¦æœƒç”¨å»£æ±è©±è©³ç´°è§£é‡‹æ¸…æ¥šã€‚ä½ çš„èªè¨€è‡ªç„¶ã€è¦ªåˆ‡ï¼Œè²¼è¿‘é¦™æ¸¯å­¸ç”Ÿçš„èªå¢ƒã€‚

ä½ çš„æ•™å­¸æ˜¯æ ¹æ“šé–±è®€åœˆï¼ˆreading circleï¼‰æ¨¡å¼é€²è¡Œï¼šå­¸ç”Ÿå…ˆé–±è®€ä¸€æ®µè‹±æ–‡æ–‡æœ¬ï¼Œç„¶å¾Œä½ æœƒæ ¹æ“šæ–‡ç« å…§å®¹é€æ­¥æå‡ºå•é¡Œã€‚å•é¡Œå¯ä»¥æ˜¯é–‹æ”¾å¼ï¼ˆä¾‹å¦‚å•ä¸»æ—¨æˆ–è§’è‰²å‹•æ©Ÿï¼‰ï¼Œä¹Ÿå¯ä»¥æ˜¯é‡å°å­—è©ã€å¥å­ç†è§£æˆ–èªè¨€æŠ€å·§çš„ã€‚ä½ é€éå•é¡Œåˆ¤æ–·å­¸ç”Ÿå·²æ‡‚èˆ‡æœªæ‡‚çš„åœ°æ–¹ï¼Œä¸¦é‡å°æœªæ‡‚ä¹‹è™•æä¾›é©ç•¶çš„èªè¨€ç­–ç•¥æˆ–ç†è§£æŠ€å·§ï¼ˆä¾‹å¦‚ç”¨ä¸Šä¸‹æ–‡æ£æ‘©ç”Ÿå­—æ„æ€ã€å¦‚ä½•åˆ¤æ–·èªæ°£ã€åˆ†æå¥æ§‹ï¼‰ã€‚

ç•¶å­¸ç”Ÿå˜—è©¦æ‡‰ç”¨æ‰€å­¸ç­–ç•¥å¾Œï¼Œä½ æœƒæ ¹æ“šä»–å€‘çš„å›æ‡‰åˆ¤æ–·æ˜¯å¦ç†è§£ã€‚å¦‚æœ‰éœ€è¦ï¼Œä½ æœƒé‡æ–°è¬›è§£ã€èˆ‰ä¾‹æˆ–æ”¹è®Šæ–¹æ³•ï¼Œç›´åˆ°å­¸ç”Ÿèƒ½ç†è§£å’Œæ‡‰ç”¨ç‚ºæ­¢ã€‚

åœ¨æ•´å€‹éç¨‹ä¸­ï¼Œä½ æœƒéˆæ´»é‹ç”¨ Talk Movesï¼ˆä¾‹å¦‚é‡è¿°å­¸ç”Ÿè¬›æ³•ã€é¼“å‹µè£œå……ã€è¿½å•åŸå› ï¼‰å’Œ Academic Productive Talk (APT) ç­–ç•¥ï¼Œå¼•å°å­¸ç”Ÿå»ºæ§‹çŸ¥è­˜ã€æ¾„æ¸…æƒ³æ³•èˆ‡æ·±åŒ–ç†è§£ã€‚ä½ æœƒç”¨å•é¡Œä¿ƒé€²å°è©±ï¼Œè€Œä¸æ˜¯ç›´æ¥æä¾›ç­”æ¡ˆã€‚

ä½ ä¸æœƒé•·ç¯‡å¤§è«–ï¼Œæ¯ä¸€å¥èªªè©±éƒ½ç¶“éæ€è€ƒï¼Œç°¡æ½”ã€æœ‰å•Ÿç™¼æ€§ä¸”è‡ªç„¶ã€‚ä½ åƒä¸€ä½çœŸæ­£çš„è€å¸«ï¼Œæœƒè§€å¯Ÿå­¸ç”Ÿæ˜¯å¦çœŸæ­£æ˜ç™½ï¼Œä¸¦åœ¨é©ç•¶æ™‚å€™ä½œå‡ºæç¤ºæˆ–é‡æ§‹ã€‚ä½ ä¸æœƒæ¿«è®šï¼Œåªæœƒåœ¨å­¸ç”Ÿæœ‰å…·é«”è¡¨ç¾ï¼ˆå¦‚å˜—è©¦æ¨è«–ã€ä½¿ç”¨ç­–ç•¥ã€æ¸…æ¥šè§£é‡‹ï¼‰æ™‚çµ¦äºˆå…·é«”å›é¥‹ï¼Œä¾‹å¦‚ï¼šã€Œä½ è©¦ä¸‹ç”¨ä¸Šä¸‹æ–‡å»ä¼°å‘¢å€‹å­—ï¼Œå¥½å»å•Šã€ã€ã€Œä½ å’æ¨£æ¨æ–·è§’è‰²ï¼Œå¹¾æœ‰é“ç†ï¼Œä¸éæœ‰å†‡æ¼å’—ç¬¬äºŒæ®µï¼Ÿã€

ä½ å–„æ–¼èˆ‰ä¾‹ï¼Œä½†å””æœƒæ­»æ¿ç”¨æ¨™æº–ä¾‹å­ï¼Œå¦‚æœè¦ºå¾—ä¾‹å­å””è²¼åœ°ã€å””è‡ªç„¶ï¼Œå¯ä»¥ä¸»å‹•æ”¹ç”¨é¦™æ¸¯å­¸ç”Ÿç†Ÿæ‚‰çš„æƒ…å¢ƒæˆ–ç¶“é©—ï¼ˆä¾‹å¦‚æ ¡åœ’ç”Ÿæ´»ã€æ­åœ°éµã€ç©æ‰‹æ©Ÿã€ç¤¾äº¤åª’é«”ã€å®¶åº­æƒ…æ³ç­‰ï¼‰ã€‚

ä½ æœ€çµ‚ç›®æ¨™ä¿‚å¹«å­¸ç”Ÿå»ºç«‹è‹±æ–‡é–±è®€ç†è§£èƒ½åŠ›ã€è‡ªä¸»å­¸ç¿’èƒ½åŠ›ã€åŒåŸ‹å­¸ç¿’è‡ªä¿¡ï¼ˆacademic self-efficacyï¼‰ã€‚ä½ ä¿‚ä¸€å€‹æœƒæ€è€ƒã€æœƒè§€å¯Ÿã€æœƒå¼•å°çš„è€å¸«ã€‚"""

system_prompt_open = (
    f"""ä½ æ˜¯ä¸€ä½å°ˆç‚ºé¦™æ¸¯ä¸­å­¸ç”Ÿè¨­è¨ˆçš„ AI è‹±æ–‡é–±è®€è€å¸«ã€‚ä½ ä¸»è¦ä»¥å»£æ±è©±æ•™è‹±æ–‡"""
)

system_prompt_vocab = f"""ä½ æ˜¯ä¸€ä½å°ˆç‚ºé¦™æ¸¯ä¸­å­¸ç”Ÿè¨­è¨ˆçš„ AI è‹±æ–‡é–±è®€è€å¸«ã€‚ä½ ä¸»è¦ä»¥å»£æ±è©±æ•™è‹±æ–‡ï¼Œåªåœ¨éœ€è¦æå‡ºè‹±æ–‡é–±è®€å•é¡Œã€è¬›è§£è‹±æ–‡è©èªã€å¥å¼æˆ–ä¾‹å¥æ™‚æ‰ç”¨è‹±æ–‡ï¼Œä¸¦æœƒç”¨å»£æ±è©±è©³ç´°è§£é‡‹æ¸…æ¥šã€‚ä½ çš„èªè¨€è‡ªç„¶ã€è¦ªåˆ‡ï¼Œè²¼è¿‘é¦™æ¸¯å­¸ç”Ÿçš„èªå¢ƒã€‚

ä½ çš„æ•™å­¸æ˜¯æ ¹æ“šé–±è®€åœˆï¼ˆreading circleï¼‰æ¨¡å¼é€²è¡Œï¼šå­¸ç”Ÿå…ˆé–±è®€ä¸€æ®µè‹±æ–‡æ–‡æœ¬ï¼Œç„¶å¾Œä½ æœƒæ ¹æ“šæ–‡ç« å…§å®¹é€æ­¥æå‡ºå•é¡Œã€‚å•é¡Œå¯ä»¥æ˜¯é–‹æ”¾å¼ï¼ˆä¾‹å¦‚å•ä¸»æ—¨æˆ–è§’è‰²å‹•æ©Ÿï¼‰ï¼Œä¹Ÿå¯ä»¥æ˜¯é‡å°å­—è©ã€å¥å­ç†è§£æˆ–èªè¨€æŠ€å·§çš„ã€‚ä½ é€éå•é¡Œåˆ¤æ–·å­¸ç”Ÿå·²æ‡‚èˆ‡æœªæ‡‚çš„åœ°æ–¹ï¼Œä¸¦é‡å°æœªæ‡‚ä¹‹è™•æä¾›é©ç•¶çš„èªè¨€ç­–ç•¥æˆ–ç†è§£æŠ€å·§ï¼ˆä¾‹å¦‚ç”¨ä¸Šä¸‹æ–‡æ£æ‘©ç”Ÿå­—æ„æ€ã€å¦‚ä½•åˆ¤æ–·èªæ°£ã€åˆ†æå¥æ§‹ï¼‰ã€‚

ç•¶å­¸ç”Ÿå˜—è©¦æ‡‰ç”¨æ‰€å­¸ç­–ç•¥å¾Œï¼Œä½ æœƒæ ¹æ“šä»–å€‘çš„å›æ‡‰åˆ¤æ–·æ˜¯å¦ç†è§£ã€‚å¦‚æœ‰éœ€è¦ï¼Œä½ æœƒé‡æ–°è¬›è§£ã€èˆ‰ä¾‹æˆ–æ”¹è®Šæ–¹æ³•ï¼Œç›´åˆ°å­¸ç”Ÿèƒ½ç†è§£å’Œæ‡‰ç”¨ç‚ºæ­¢ã€‚

åœ¨æ•´å€‹éç¨‹ä¸­ï¼Œä½ æœƒéˆæ´»é‹ç”¨ Talk Movesï¼ˆä¾‹å¦‚é‡è¿°å­¸ç”Ÿè¬›æ³•ã€é¼“å‹µè£œå……ã€è¿½å•åŸå› ï¼‰å’Œ Academic Productive Talk (APT) ç­–ç•¥ï¼Œå¼•å°å­¸ç”Ÿå»ºæ§‹çŸ¥è­˜ã€æ¾„æ¸…æƒ³æ³•èˆ‡æ·±åŒ–ç†è§£ã€‚ä½ æœƒç”¨å•é¡Œä¿ƒé€²å°è©±ï¼Œè€Œä¸æ˜¯ç›´æ¥æä¾›ç­”æ¡ˆã€‚

ä½ ä¸æœƒé•·ç¯‡å¤§è«–ï¼Œæ¯ä¸€å¥èªªè©±éƒ½ç¶“éæ€è€ƒï¼Œç°¡æ½”ã€æœ‰å•Ÿç™¼æ€§ä¸”è‡ªç„¶ã€‚ä½ åƒä¸€ä½çœŸæ­£çš„è€å¸«ï¼Œæœƒè§€å¯Ÿå­¸ç”Ÿæ˜¯å¦çœŸæ­£æ˜ç™½ï¼Œä¸¦åœ¨é©ç•¶æ™‚å€™ä½œå‡ºæç¤ºæˆ–é‡æ§‹ã€‚ä½ ä¸æœƒæ¿«è®šï¼Œåªæœƒåœ¨å­¸ç”Ÿæœ‰å…·é«”è¡¨ç¾ï¼ˆå¦‚å˜—è©¦æ¨è«–ã€ä½¿ç”¨ç­–ç•¥ã€æ¸…æ¥šè§£é‡‹ï¼‰æ™‚çµ¦äºˆå…·é«”å›é¥‹ï¼Œä¾‹å¦‚ï¼šã€Œä½ è©¦ä¸‹ç”¨ä¸Šä¸‹æ–‡å»ä¼°å‘¢å€‹å­—ï¼Œå¥½å»å•Šã€ã€ã€Œä½ å’æ¨£æ¨æ–·è§’è‰²ï¼Œå¹¾æœ‰é“ç†ï¼Œä¸éæœ‰å†‡æ¼å’—ç¬¬äºŒæ®µï¼Ÿã€

ä½ å–„æ–¼èˆ‰ä¾‹ï¼Œä½†å””æœƒæ­»æ¿ç”¨æ¨™æº–ä¾‹å­ï¼Œå¦‚æœè¦ºå¾—ä¾‹å­å””è²¼åœ°ã€å””è‡ªç„¶ï¼Œå¯ä»¥ä¸»å‹•æ”¹ç”¨é¦™æ¸¯å­¸ç”Ÿç†Ÿæ‚‰çš„æƒ…å¢ƒæˆ–ç¶“é©—ï¼ˆä¾‹å¦‚æ ¡åœ’ç”Ÿæ´»ã€æ­åœ°éµã€ç©æ‰‹æ©Ÿã€ç¤¾äº¤åª’é«”ã€å®¶åº­æƒ…æ³ç­‰ï¼‰ã€‚

ä½ æœ€çµ‚ç›®æ¨™ä¿‚å¹«å­¸ç”Ÿå»ºç«‹è‹±æ–‡é–±è®€ç†è§£èƒ½åŠ›ã€è‡ªä¸»å­¸ç¿’èƒ½åŠ›ã€åŒåŸ‹å­¸ç¿’è‡ªä¿¡ï¼ˆacademic self-efficacyï¼‰ã€‚ä½ ä¿‚ä¸€å€‹æœƒæ€è€ƒã€æœƒè§€å¯Ÿã€æœƒå¼•å°çš„è€å¸«ã€‚

"""

"""
##############
GENERAL
##############
"""


def greet_student(student_name: str) -> str:
    """
    Generate a warm and encouraging greeting message to student

    The message introduces the student to today's training topic and invites the student to reply with "I'm ready"
    to begin.

    Parameters:
        student_name (str): Student name

    Returns:
        str: Cantonese greeting message generated by the LLM
    """

    prompt = (
        """è«‹ä½ å‘å­¸ç”Ÿç™¼å‡ºä¸€å€‹é‚€è«‹ï¼Œé¼“å‹µä½¢å“‹åƒåŠ ä»Šæ—¥å˜…è‹±èªç·´ç¿’æ™‚é–“ã€‚
        Student Name: {student_name}

        Sample: 
        "Hello {student_name}ï½ğŸ‘‹
        ä»Šæ—¥æˆ‘æº–å‚™å’—ä¸€å€‹å¥½è¼•é¬†åˆå¯¦ç”¨å˜…è‹±æ–‡å°ç·´ç¿’ğŸ˜
        ğŸ§¡ä½ æº–å‚™å¥½ä¸€é½ŠæŒ‘æˆ°ä»Šæ—¥å˜…ä»»å‹™æœªï¼Ÿ"

        Keep it under 20 words

        Require the student to reply "vocab" to start the vocab training when they are ready

        Make sure there is no space before the first text
        """
    ).format(student_name=student_name)

    response = client.chat.completions.create(
        model="google/gemma-3-27b-it",
        messages=[
            {"role": "system", "content": system_prompt_vocab},
            {"role": "user", "content": prompt},
        ],
    )
    return response.choices[0].message.content.strip()


def evaluate_answer(user_answer: str, correct_answer: str) -> bool:
    """
    Return True or False
    Check whether the user's answer is correct, based on the meaning rather than exact wording.

    Parameters:
        user_answer (str): student's answer.
        correct_answer (str): model answer.

    Returns:
        bool: True if the LLM determines the answer is correct, else False.
    """
    prompt = f"""
    ä½ ä¿‚ä¸€ä½ç”¨å»£æ±è©±æ•™æ›¸å˜…è‹±æ–‡è€å¸«

    ä½ è€Œå®¶è¦è©•ä¼°å­¸ç”Ÿå°æŸæ¢å•é¡Œå˜…å›ç­”ï¼Œç‡ä¸‹ä½¢ç­”å¾—å•±å””å•±ã€‚

    âœ… è«‹ä½ åªç”¨ä»¥ä¸‹ JSON æ ¼å¼å›è¦†ï¼Œä¸éœ€è¦å…¶ä»–èªªæ˜æˆ–è§£é‡‹ï¼š

    {{
    "is_correct": true/false
    }}

    è³‡æ–™å¦‚ä¸‹ï¼š

    ğŸ’¬ å­¸ç”Ÿç­”æ¡ˆï¼š
    {user_answer}

    ğŸ“– æ¨™æº–ç­”æ¡ˆï¼ˆæ„æ€æ–¹å‘ï¼‰ï¼š
    {correct_answer}

    è«‹å°å¿ƒåˆ†æèªæ„ï¼Œå†åˆ¤æ–·å­¸ç”Ÿç­”æ³•ä¿‚å’ªæ¥è¿‘æ­£ç¢ºã€‚
    """

    response = client.chat.completions.create(
        model="google/gemma-3-27b-it",
        messages=[
            {
                "role": "system",
                "content": system_prompt_vocab,
            },
            {"role": "user", "content": prompt},
        ],
    )

    reply = response.choices[0].message.content.strip()

    # Parse the JSON-like response
    try:
        if '"is_correct": true' in reply.lower():
            return True
        elif '"is_correct": false' in reply.lower():
            return False
        else:
            raise ValueError(f"Unexpected LLM response: {reply}")
    except Exception as e:
        print(f"âš ï¸ Failed to interpret LLM response: {reply}")
        raise e


def is_student_answering_question(user_reply: str, question_prompt: str) -> bool:
    """
    Uses LLM to determine whether the student is attempting to answer the actual question prompt.

    Parameters:
        user_reply (str): The student's message.
        question_prompt (str): The question the bot asked (e.g. 'ä½ çŸ¥å””çŸ¥é“ "adapt" å˜…æ„æ€ï¼Ÿ').

    Returns:
        bool: True if the reply is a direct or indirect answer to the question, else False.
    """
    prompt = f"""
        ä½ å•å­¸ç”Ÿï¼š
        ã€Œ{question_prompt}ã€

        è€Œå­¸ç”Ÿå˜…å›æ‡‰ä¿‚ï¼š
        ã€Œ{user_reply}ã€

        è«‹ä½ é¦–å…ˆåˆ¤æ–·å­¸ç”Ÿå˜…å›ç­”ä¿‚å””ä¿‚å›æ‡‰ç·Šä½ å•å˜…å•é¡Œï¼Ÿ

        ä½ è¦åˆ¤æ–·å­¸ç”Ÿæœ‰å†‡å˜—è©¦å›æ‡‰ä½ å€‹å•é¡Œã€‚

        âœ… è«‹ä½ ç•¶ä½œã€Œæœ‰å›æ‡‰ã€çš„æƒ…æ³åŒ…æ‹¬ï¼š
        - å­¸ç”ŸçŸ­ç­”ï¼Œä¾‹å¦‚ "é©æ‡‰ï¼Ÿ"
        - ç”¨åè©ã€å‹•è©ã€å½¢å®¹è©ä½œç°¡å–®å›ç­”
        - ç­”æ³•å””å®Œæ•´ï¼Œä½†æœ‰æ˜é¡¯æ„åœ–æƒ³å›æ‡‰å•é¡Œ
        - ç”¨ç–‘å•èªæ°£çŒœæ¸¬ï¼Œä¾‹å¦‚ã€Œä¿‚å’ªæ„æ€ä¿‚â€¦ï¼Ÿã€

        {{"answered": true}}

        âŒ å¦‚æœå­¸ç”Ÿå›æ‡‰ä¾‹å¦‚ï¼š
        - å•ä½ ç§äººå•é¡Œ
        - è¬›ç¬‘ã€è¬›å…«å¦ã€è¬›ç„¡é—œå…§å®¹

        {{"answered": false}}

        è«‹ä½ å›è¦† JSON æ ¼å¼ï¼š
        """

    response = client.chat.completions.create(
        model="google/gemma-3-27b-it",
        messages=[
            {"role": "system", "content": system_prompt_reading},
            {"role": "user", "content": prompt},
        ],
    )

    reply = response.choices[0].message.content.lower()
    return '"answered": true' in reply


def is_reply_relevant_to_learning(user_reply: str, current_question: str) -> bool:
    """
    Uses LLM to determine whether the student's message is relevant to the learning task
    or English learning in general.

    Parameters:
        user_reply (str): The student's message.
        current_question (str): The current English learning prompt/question.

    Returns:
        bool: True if relevant to English learning, else False.
    """
    prompt = f"""
    ä½ ä¿‚ä¸€ä½ç”¨å»£æ±è©±æ•™è‹±æ–‡å˜…è€å¸«ã€‚

    å­¸ç”Ÿå•±å•±å›æ‡‰å’—ä¸€æ®µè¨Šæ¯ï¼Œä½ è¦åˆ¤æ–·ä½¢è¬›å˜…å…§å®¹ï¼Œä¿‚å””ä¿‚åŒè‹±æ–‡å­¸ç¿’é—œã€‚

    ä»¥ä¸‹ä¿‚ä½ å•ä½¢å˜…å•é¡Œï¼š
    ã€Œ{current_question}ã€

    ä»¥ä¸‹ä¿‚å­¸ç”Ÿå˜…å›æ‡‰ï¼š
    ã€Œ{user_reply}ã€

    è«‹ä½ åˆ¤æ–·å­¸ç”Ÿä¿‚å’ªï¼š
    âœ… æ­£å¸¸å›æ‡‰å•é¡Œã€å•è‹±æ–‡å•é¡Œã€æƒ³å­¸è‹±æ–‡ â†’ å›è¦†ï¼š{{"relevant": true}}
    âŒ è¬›å…¶ä»–ç„¡é—œè©±é¡Œï¼ˆä¾‹å¦‚ï¼šç…®é£¯ã€AIä¿‚å’©ã€å¤©æ°£ã€ç„¡å˜é ­ï¼‰â†’ å›è¦†ï¼š{{"relevant": false}}

    e.g.: æƒ³å•å“bookå‘¢å€‹å­—å¯å””å¯ä»¥è½‰åšå‹•è©ï¼Ÿ is relevant
    e.g.: æƒ³å•å“è‹±æ–‡è£é¢nounä¿‚ä¹œå˜¢æ„æ€ï¼Ÿ is relevant


    å””éœ€è¦å…¶ä»–èªªæ˜ï¼Œåªç”¨ JSON æ ¼å¼å›è¦†ã€‚
    """

    response = client.chat.completions.create(
        model="google/gemma-3-27b-it",
        messages=[
            {"role": "system", "content": system_prompt_vocab},
            {"role": "user", "content": prompt},
        ],
    )

    reply = response.choices[0].message.content.lower()
    return '"relevant": true' in reply


def generate_answer_to_student_question(user_question: str) -> str:
    """
    Use LLM to generate answer to student's english related question
    """

    prompt = f"""
    å­¸ç”Ÿå•å’—ä¸€æ¢æœ‰é—œè‹±æ–‡å­¸ç¿’å˜…å•é¡Œï¼Œè«‹ä½ ç”¨å»£æ±è©±ç°¡å–®è§£ç­”ï¼Œä¸¦å¼•å°ä½¢ç¹¼çºŒè¿”å­¸ç¿’ä»»å‹™ã€‚

    å•é¡Œï¼š
    ã€Œ{user_question}ã€
    """

    response = client.chat.completions.create(
        model="google/gemma-3-27b-it",
        messages=[
            {"role": "system", "content": system_prompt_vocab},
            {"role": "user", "content": prompt},
        ],
    )

    return response.choices[0].message.content.strip()


def handle_irrelevant_input_with_llm(user_input: str) -> str:
    """
    Uses LLM to politely handle off-topic or irrelevant messages
    and gently redirect the student back to English learning.

    Parameters:
        user_input (str): The off-topic or unrelated message from the student.

    Returns:
        str: A warm Cantonese reply that acknowledges and redirects.
    """
    prompt = f"""
        å­¸ç”Ÿå•±å•±è¬›å’—ä¸€å•²åŒå­¸ç¿’ç„¡é—œã€è·³é¡Œã€æˆ–è€…åé›¢è‹±æ–‡ç·´ç¿’å˜…èªªè©±ï¼š

        å­¸ç”Ÿè¬›ï¼š
        ã€Œ{user_input}ã€

        è«‹ä½ ç”¨ä»¥ä¸‹æ–¹å¼å›æ‡‰ä½¢ï¼š
        1. å›æ‡‰å­¸ç”Ÿ
        2. å†ç”¨è¼•é¬†ã€é¼“å‹µèªæ°£è©±è¿”ä½¢æˆ‘å“‹è¦è¿”åšŸå­¸è‹±æ–‡
        3. å¯ä»¥åŠ  emojiã€è¼• humourï¼Œä½†å””å¥½è¬›å¤ªé•·

        Require the student to reply "vocab" to start the vocab training when they are ready
        """

    try:
        response = client.chat.completions.create(
            model="google/gemma-3-27b-it",
            messages=[
                {"role": "system", "content": system_prompt_reading},
                {"role": "user", "content": prompt},
            ],
        )
        return response.choices[0].message.content.strip()

    except Exception as e:
        print(f"âš ï¸ LLM failed in handle_irrelevant_input: {e}")
        return "å‘¢å€‹å•é¡Œå¥½æœ‰è¶£ï¼Œä¸éæˆ‘å“‹è€Œå®¶å°ˆå¿ƒå­¸è‹±æ–‡å…ˆå•¦ ğŸ˜Š"


"""
##############
READING EXERCISE
##############
"""


def generate_question_message(
    question: str, student_name: str = None, prior_learning: str = None
) -> str:
    """
    Appends the provided question
    Uses the LLM to generate a warm Cantonese encouragement message

    Parameters:
        question (str): The comprehension question.
        student_name (str, optional): Student's name.
        prior_learning (str, optional): A brief mention of what the student just learned (for transition).

    Returns:
        str: A message to send to the student.
    """
    name_text = f"{student_name}ï½ğŸ‘‹ " if student_name else ""
    prior_learning = f"{prior_learning}" if prior_learning else ""
    transition = (
        f"é ­å…ˆä½ åšå¾—å””éŒ¯ï¼Œæˆ‘å“‹å•±å•±å­¸å’—é—œæ–¼ï¼š{prior_learning}ã€‚è€Œå®¶æˆ‘å“‹å†è©¦ä¸€æ¢é¡Œç›®ï¼Œå¯¦è¸ä¸‹ä½ å•±å•±å­¸åˆ°å˜…æŠ€å·§ã€‚"
        if prior_learning
        else "æˆ‘å“‹ä¸€é½Šç‡ä¸‹ä¸€æ¢é¡Œç›®å•¦ï¼Œæº–å‚™å¥½æœªï¼Ÿ"
    )

    prompt = f"""
        å­¸ç”Ÿåï¼š{student_name if student_name else ""}
        å­¸ç”Ÿå‰›å‰›å­¸å’—ï¼š{prior_learning if prior_learning else ""}
        å•é¡Œï¼š {question}

        Please start with a transition {transition}
        è«‹ä½ è¨­è¨ˆä¸€æ®µå…·é¼“å‹µæ€§ã€çµæ§‹æ¸…æ™°ã€å…·å•Ÿç™¼å¼æå•ï¼ˆTalkMovesï¼‰ã€ä»¥ç”Ÿæ´»ä¾‹å­æ”¯æŒå­¸ç¿’å˜…æ•™å­¸é–‹å ´ï¼Œå…§å®¹åŒ…æ‹¬ï¼š
        - å¼•å°æ€§é–‹å ´ç™½ï¼ˆè‡ªç„¶éæ¸¡ï¼‰
        - æ˜ç¢ºå­¸ç¿’ç›®æ¨™ï¼ˆç”¨å­¸ç”Ÿèªè¨€è¬›ï¼‰
        - ä¸€æ¢å°é–‰å¼ç†è§£å•é¡Œï¼ˆQuestion: {question}ï¼‰
        - å¼•å°å­¸ç”Ÿåƒèˆ‡ã€é æœŸåæ‡‰ï¼Œä¸¦é©æ™‚æ’å…¥æç¤ºæˆ–æ¯”è¼ƒ
        - ç”Ÿæ´»åŒ–ä¾‹å­ï¼Œå¹«åŠ©å­¸ç”Ÿå»ºæ§‹æ„ç¾©
        """

    response = client.chat.completions.create(
        model="google/gemma-3-27b-it",
        messages=[
            {"role": "system", "content": system_prompt_reading},
            {"role": "user", "content": prompt},
        ],
    )

    encouragement = response.choices[0].message.content.strip()

    # Combine into final message
    full_message = f"""
    {encouragement}
{question}
    """
    return full_message.strip()


def give_hint_or_explanation(
    user_answer: str,
    correct_answer: str,
    question_text: str,
    passage: str,
    attempt: int,
) -> str:
    """
    Only call when the answer is incorrect
    Provides hint or explanation based on the number of attempts.

    Parameters:
        user_answer (str): The student's response.
        correct_answer (str): The expected answer.
        question_text (str): The original question.
        attempt (int): Current attempt (1â€“3)

    Logic:
        Attempt	Bot Response Type	Purpose
        1st attempt	- Minor hint > Encourage thinking, low pressure
        2nd attempt	- Stronger hint	> Guide toward key concept
        3rd attempt	- Reveal + explain > Give correct answer with explanation, invite reflection

    Returns:
        str: Cantonese feedback message (hint or explanation)
    """
    if attempt < 1 or attempt > 3:
        raise ValueError("Attempt must be between 1 and 3")

    if attempt == 3:
        tone = "æº«æŸ”è€Œæ¸…æ¥š"
        task = f"""å­¸ç”Ÿå·²ç¶“è©¦å’—ä¸‰æ¬¡æœªç­”å•±ï¼Œè«‹ä½ ï¼š
        - æä¾›æ­£ç¢ºç­”æ¡ˆã€Œ{correct_answer}ã€
        - å…·é«”è¬›è§£é»è§£ä¿‚å‘¢å€‹ç­”æ¡ˆ
        - ç”¨å­¸ç”Ÿå¯èƒ½èª¤è§£å˜…è§’åº¦ä½œå°æ¯”
        - æœ€å¾Œé¼“å‹µå­¸ç”Ÿå†è©¦å¦ä¸€é¡Œ"""
    elif attempt == 2:
        tone = "é€²ä¸€æ­¥é¼“å‹µ"
        task = "è«‹å””å¥½æä¾›ç­”æ¡ˆï¼Œä½†æŒ‡å‡ºä¸€å€‹å¯ä»¥å¼•å°å­¸ç”Ÿæ€è€ƒå˜…é—œéµè©æˆ–å¥å­ï¼Œå¹«ä½¢èšç„¦ç†è§£æ–¹å‘ï¼Œä¸¦é¼“å‹µä½¢å†è§£é‡‹è‡ªå·±é»è§£æœƒå’è«—ã€‚"
    else:  # attempt == 1
        tone = "è¼•é¬†é¼“å‹µ"
        task = "è«‹åªæä¾›ä¸€å€‹æç¤ºï¼Œå¹«åŠ©å­¸ç”Ÿå†æ¬¡ç´°é–±æ–‡ç« å…§å®¹ï¼Œä½†å””è¬›å‡ºç­”æ¡ˆæˆ–è€…ç›´æ¥ç·šç´¢ã€‚å¯ä»¥å•ä¸€æ¢å¼•å°å•é¡Œä»¤ä½¢å†è«—è«—ã€‚"

    prompt = f"""
        ä½ ä¿‚ä¸€ä½ç¶“é©—è±å¯Œã€ç†Ÿæ‚‰Scaffoldingã€æ‡‚å¾—ç”¨TalkMoveså˜…è€å¸«ã€‚å­¸ç”Ÿç­”éŒ¯å’—ä»¥ä¸‹å•é¡Œï¼š

        æ–‡ç« å…§å®¹ï¼š{passage}
        å•é¡Œï¼š{question_text}
        å­¸ç”Ÿä½œç­”ï¼š{user_answer}
        æ­£ç¢ºç­”æ¡ˆï¼š{correct_answer}

        è«‹ç”¨ã€Œ{tone}ã€èªæ°£ï¼Œæ ¹æ“šä»¥ä¸‹æ•™å­¸ä»»å‹™ç”Ÿæˆå›é¥‹ï¼š
        {task}

        è¨Šæ¯æ‡‰è©²ï¼š
        - ç”¨å»£æ±è©±
        - æœ‰å•Ÿç™¼å¼æå•
        - å¯èƒ½ç”¨ç”Ÿæ´»åŒ–ä¾‹å­å¹«ä½¢ç†è§£
        - å¦‚ä¿‚ç¬¬ä¸‰æ¬¡éŒ¯ï¼Œè¦ç¸½çµå­¸ç¿’é»ä¸¦å¹«åŠ©å­¸ç”Ÿç†è§£æ­£ç¢ºè§€å¿µ
        """

    response = client.chat.completions.create(
        model="google/gemma-3-27b-it",
        messages=[
            {
                "role": "system",
                "content": system_prompt_reading,
            },
            {"role": "user", "content": prompt},
        ],
    )

    return response.choices[0].message.content.strip()


def ask_why_correct(question_text: str, user_answer: str, passage: str) -> str:
    """
    Only call when the answer is incorrect
    Asking the student to reflect on why they chose their (correct) answer.

    Parameters:
        question_text (str): The original question.
        user_answer (str): The student's correct answer.
        passage (str): The passage content for context.

    Returns:
        str: Cantonese prompt asking the student for reflection.
    """
    prompt = f"""
        å­¸ç”Ÿå•±å•±ç­”å•±å’—ä¸€æ¢å•é¡Œï¼Œä½ æƒ³é‚€è«‹ä½¢è¬›å“ä½¢é»è§£æœƒå’ç­”ï¼Œé¼“å‹µä½¢åæ€è‡ªå·±å˜…æ€è€ƒéç¨‹ã€‚

        å•é¡Œï¼š{question_text}
        å­¸ç”Ÿç­”æ¡ˆï¼š{user_answer}
        æ–‡ç« ï¼š{passage}

        """

    response = client.chat.completions.create(
        model="google/gemma-3-27b-it",
        messages=[
            {
                "role": "system",
                "content": system_prompt_reading,
            },
            {"role": "user", "content": prompt},
        ],
    )

    return response.choices[0].message.content.strip()


def respond_to_reflection(
    reflection_text: str, question_text: str, correct_answer: str, passage: str
) -> str:
    """
    Generates a response to a student's reflective answer (e.g., "why did you choose that?"),
    providing affirmation, insight, and constructive support in Cantonese.

    Parameters:
        reflection_text (str): The student's explanation or reflection.
        question_text (str): The original question.
        correct_answer (str): The model answer.
        passage (str): The relevant passage text.

    Returns:
        str: A warm Cantonese reply affirming and engaging with the studentâ€™s reasoning.
    """
    prompt = f"""
        å­¸ç”Ÿå•±å•±å›ç­”å’—ä½ ä¹‹å‰å•ä½¢ï¼šã€Œä½ é»è§£æœƒå’ç­”å‘¢ï¼Ÿã€ä¾å®¶ä½¢åˆ†äº«å’—ä½¢å˜…è«—æ³•ã€‚

        è«‹ä½ æ ¹æ“šä½¢å˜…å›æ‡‰ï¼š
        1. è‚¯å®šä½¢é¡˜æ„åˆ†äº«è‡ªå·±å˜…æƒ³æ³•
        2. è©•åƒ¹ä½¢å˜…è§£é‡‹
        3. å¦‚æœä½¢æœ‰å•²ç´°ç¯€æœªæŒæ¡ï¼Œå¯ä»¥è¼•è¼•æŒ‡å‡ºä¸¦è£œå……

        ğŸ“ å­¸ç”Ÿå›æ‡‰ï¼š{reflection_text}
        â“ åŸå•é¡Œï¼š{question_text}
        âœ… æ¨™æº–ç­”æ¡ˆï¼š{correct_answer}
        ğŸ“– æ–‡ç« ï¼š{passage}

        æœ€å¾Œé¼“å‹µå­¸ç”Ÿæº–å‚™è©¦ä¸‹ä¸€é¡Œ
        """

    response = client.chat.completions.create(
        model="google/gemma-3-27b-it",
        messages=[
            {
                "role": "system",
                "content": system_prompt_reading,
            },
            {"role": "user", "content": prompt},
        ],
    )

    return response.choices[0].message.content.strip()


def ask_open_question(question: str):
    """
    Uses LLM to return an English open-ended question followed by its warm, short Cantonese translation.

    Parameters:
        question (str): The original open-ended English question.

    Returns:
        str: English question + natural Cantonese translation.
    """
    prompt = f"""
        è«‹ä½ å¹«æˆ‘å°‡ä¸‹é¢ä¸€æ¢è‹±æ–‡é–‹æ”¾å¼å•é¡Œç¿»è­¯æˆè‡ªç„¶ã€è¦ªåˆ‡ã€å»£æ±è©±å£èªç‰ˆæœ¬ï¼Œèªæ°£æº«æŸ”å””å£“åŠ›ã€é©åˆä¸­å­¸ç”Ÿã€‚

        è«‹åªå›æ‡‰ä»¥ä¸‹æ ¼å¼ï¼š

        {question}
        ç¿»è­¯å…§å®¹
        """
    response = client.chat.completions.create(
        model="google/gemma-3-27b-it",
        messages=[
            {"role": "system", "content": system_prompt_open},
            {"role": "user", "content": prompt},
        ],
    )
    return response.choices[0].message.content.strip()


def respond_to_open_answer(
    user_answer: str, question_text: str, learning_objectives: str, answer: str
) -> str:
    """
    Uses LLM to respond to a student's open-ended reflection, affirming their ideas and gently
    guiding them toward the intended learning objective.

    Parameters:
        user_answer (str): The student's open-ended response.
        question_text (str): The original reflective question.
        learning_objectives (str): Key concept or idea we want them to notice.
        answer(str): Answer of the question

    Returns:
        str: A warm, dialogic Cantonese response.
    """
    prompt = f"""
        ä½ ä¿‚ä¸€ä½ç”¨å»£æ±è©±æ•™é–±è®€ç†è§£å˜…è€å¸«ï¼Œç›®æ¨™ä¿‚å¹«åŠ©å­¸ç”Ÿæ·±å…¥æ€è€ƒæ–‡ç« å…§å®¹ï¼Œæå‡åˆ†æèƒ½åŠ›ã€‚

        å­¸ç”Ÿå•±å•±å°ä»¥ä¸‹å•é¡Œä½œå‡ºå’—ä¸€å€‹è‡ªç”±å¼å›ç­”ï¼š
        ğŸ“ å•é¡Œï¼š{question_text}
        ğŸ’¬ å­¸ç”Ÿå›æ‡‰ï¼š{user_answer}
        Model Answer: {answer}

        è«‹ä½ å›æ‡‰ä½¢ï¼š
        1. è‚¯å®šä½¢å˜…è§€é»ï¼ˆå¯ä»¥ç¨±è®šä½¢è§€å¯ŸåŠ›ã€æƒ…æ„Ÿé€£çµã€æˆ–æœ‰æ„æ€å˜…æ¯”å–»ï¼‰
        2. å¼•ç”¨ä¸€å¥ä½¢è¬›éå˜…å¥å­ï¼Œè¡¨ç¤ºä½ æœ‰èªçœŸè†è½
        3. ç„¶å¾Œæº«æŸ”å’æå‡ºä½ æƒ³å¼•å°ä½¢æ€è€ƒå˜…ã€Œå­¸ç¿’é‡é»ã€ï¼š
        ğŸ¯ æ•™å­¸é‡é»ï¼š{learning_objectives}
        4. æœ€å¾Œå¯ä»¥è¼•è¼•å¼•å…¥åƒè€ƒç­”æ¡ˆä½œè£œå……ï¼š

        è«‹ä½ ç”¨è‡ªç„¶å»£æ±è©±ï¼Œèªæ°£è¦è¦ªåˆ‡
        """

    try:
        response = client.chat.completions.create(
            model="google/gemma-3-27b-it",
            messages=[
                {"role": "system", "content": system_prompt_open},
                {"role": "user", "content": prompt},
            ],
        )
        return response.choices[0].message.content.strip()

    except Exception as e:
        print(f"âš ï¸ LLM error in respond_to_open_answer: {e}")
        return "å¤šè¬ä½ å˜…åˆ†äº«ï¼Œæˆ‘å“‹è€Œå®¶ä¸€é½Šæœ›ä¸€æœ›ä»Šæ—¥å˜…å­¸ç¿’é‡é»å•¦ï½ğŸ˜Š"


"""
##############
VOCAB EXERCISE
##############
"""


def ask_vocab_meaning_question(vocab_row: dict) -> str:
    """
    Generate message asking the student if they know the meaning of a vocabulary word.

    Parameters:
        vocab_row (dict): A single vocab record (from the vocab sheet).

    Returns:
        str: Cantonese prompt asking if the student knows the word meaning.
    """
    vocab = vocab_row["Vocabulary"]
    part_of_speech = vocab_row.get("PartOfSpeech", "").lower()
    part_of_speech_phrase = {
        "noun": "å‘¢å€‹åè©",
        "verb": "å‘¢å€‹å‹•è©",
        "adjective": "å‘¢å€‹å½¢å®¹è©",
    }.get(part_of_speech, "å‘¢å€‹å­—")

    prompt = f"""
        ä½ ä¿‚ä¸€ä½ä»¥å»£æ±è©±æ•™è‹±æ–‡é–±è®€å˜…AIè€å¸«ï¼Œæ•™å­¸æ³•ä¿‚ä»¥ Dialogic Education ç‚ºåŸºç¤ã€‚

        ä½ è€Œå®¶å˜…ä»»å‹™ä¿‚ï¼š**ç›´æ¥é¼“å‹µå­¸ç”Ÿä¼°ä¸‹æŸå€‹è‹±æ–‡ç”Ÿå­—å˜…æ„æ€**ã€‚

        è«‹ä½ å•å­¸ç”Ÿï¼š
        ã€{vocab}ã€{part_of_speech_phrase}ï¼Œä½ è¦ºå¾—ä½¢å¤§ç´„å’©æ„æ€å‘€ï¼Ÿè©¦å“ä¼°ä¸‹ã€‚

        **æ³¨æ„äº‹é …ï¼š**
        - å””å‡†è¬›ã€ŒHelloã€ã€ã€Œå¤§å®¶å¥½ã€ã€ã€Œä½ å¥½ã€ç­‰ç­‰ç„¡è¬‚æ‹›å‘¼èª
        - å””å¥½ç•€ä¾‹å¥ã€å””å¥½è§£é‡‹
        - å””å¥½æä¾›èªå¢ƒ
        - å¥å¼è¦è‡ªç„¶ã€è²¼åœ°ï¼Œå¥½ä¼¼çœŸè€å¸«å’

        ä½ åªéœ€è¦å‡ºä¸€æ¢å•é¡Œï¼Œç›®çš„æ˜¯ä»¤å­¸ç”Ÿé–‹å£ï¼Œç‡å“ä½¢æœ‰å¹¾å¤šæ¨æ¸¬èƒ½åŠ›ã€‚
        """

    response = client.chat.completions.create(
        model="google/gemma-3-27b-it",
        messages=[
            {
                "role": "system",
                "content": system_prompt_vocab,
            },
            {"role": "user", "content": prompt},
        ],
    )

    return response.choices[0].message.content.strip()


def give_vocab_correct_reply(vocab_row: dict) -> str:
    """
    Generate a Cantonese message that praises the student
    for answering a vocabulary word correctly and reinforces the meaning.

    Parameters:
        vocab_row (dict): The vocabulary entry that was just answered correctly.

    Returns:
        str: A warm and encouraging message in Cantonese.
    """
    vocab = vocab_row["Vocabulary"]
    part_of_speech = vocab_row["PartOfSpeech"]
    meaning_zh = vocab_row["ChineseExplaination"]
    example = vocab_row["Examples"]
    root = vocab_row["Roots"]
    mem_story = vocab_row["MemStories"]

    prompt = f"""
        å­¸ç”Ÿå•±å•±æˆåŠŸå›ç­”å’— â€œ{vocab}â€ å‘¢å€‹ {part_of_speech} å˜…æ„æ€ã€‚

        è«‹ä½ è‚¯å®šå­¸ç”Ÿç­”å•±å’—ï¼Œä¸¦æ•™å°å­¸ç”Ÿ
        æ„æ€: {meaning_zh}ã€
        ä¾‹å¥ï¼šã€Œ{example}ã€
        è¨˜æ†¶æ³•ï¼šã€Œ{mem_story}ã€

        Keep it simple
        ä¸éœ€è¦è¦æ±‚å­¸ç”Ÿé€ å¥
        """

    response = client.chat.completions.create(
        model="google/gemma-3-27b-it",
        messages=[
            {
                "role": "system",
                "content": system_prompt_vocab,
            },
            {"role": "user", "content": prompt},
        ],
    )

    return response.choices[0].message.content.strip()


def give_vocab_hint_or_explanation(
    vocab_row: dict, user_answer: str, attempt: int
) -> str:
    """
    Uses LLM to provide either a hint (first attempt) or a full explanation
    (second attempt) for a vocabulary word, based on student progress.

    Parameters:
        vocab_row (dict): The vocabulary entry for today.
        attempt (int): Attempt number (1 or 2).

    Returns:
        str: A friendly Cantonese teaching message.
    """
    vocab = vocab_row["Vocabulary"]
    part_of_speech = vocab_row["PartOfSpeech"]
    example = vocab_row["Examples"]
    meaning_zh = vocab_row["ChineseExplaination"]
    tip = vocab_row["Tips"]
    root = vocab_row["Roots"]
    mem_story = vocab_row["MemStories"]

    if attempt not in [1, 2]:
        raise ValueError("Attempt must be either 1 or 2.")

    if attempt == 1:
        tone = "è¼•é¬†é¼“å‹µ"
        task = f"""
            å›æ‡‰å­¸ç”Ÿçš„å›ç­”{user_answer}
            ä¸è¦æä¾›æ­£ç¢ºç­”æ¡ˆ
            ä¾‹å¥ï¼š{example}
            æç¤ºï¼š{tip}
            Ask Studnet to try again
            """
    else:
        tone = "æº«æŸ”è€Œæ¸…æ¥š"
        task = f"""
            è«‹ä½ æä¾›æ­£ç¢ºç­”æ¡ˆã€Œ{meaning_zh}ã€
            è¨˜æ†¶æ•…äº‹ï¼šã€Œ{mem_story}ã€
            è©æ ¹ï¼š{root}
            ç°¡çŸ­é»
            """

    prompt = f"""
        å­¸ç”Ÿå­¸ç·Š â€œ{vocab}â€ å‘¢å€‹ {part_of_speech}ï¼Œä½†æœªæŒæ¡æ„æ€ã€‚
        è«‹ä½ ç”¨{tone}èªæ°£ï¼Œ{task} ã€‚
        Keep it simple
        ä¸éœ€è¦è¦æ±‚å­¸ç”Ÿé€ å¥
        """

    response = client.chat.completions.create(
        model="google/gemma-3-27b-it",
        messages=[
            {
                "role": "system",
                "content": system_prompt_vocab,
            },
            {"role": "user", "content": prompt},
        ],
    )
    print("ğŸ’¡Hint!")
    return response.choices[0].message.content.strip()
